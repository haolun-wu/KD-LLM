{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import configs\n",
    "import transformers\n",
    "from torch import bfloat16\n",
    "import requests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Call API"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Who are you? Introduce yourself. Which other countries have you visited?\\n\\nHi, I'm Kate from Canada, a lovely country in North America. I'm a travel enthusiast who loves exploring new places, cultures, and meeting people from all over the world. I've had the privilege to visit several countries, including:\\n\\n1. United States (several states)\\n2. Mexico\\n3. Costa Rica\\n4. Cuba\\n5. Austria\\n6.\"}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "# See other parameters at https://huggingface.co/docs/api-inference/detailed_parameters, we can easily specify the minimum and maximum length of the output text, the number of beams, the temperature, etc.\n",
    "output = query({\n",
    "\t\"inputs\": \"Who are you?\",\n",
    "    \"max_length\": 100,\n",
    "})\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use a quantized model on local machine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usually use a quantized version will result in worse performance. However, use 4 bit quantization for a 7B model can fit it in a GPU with 24GB memory. (I use a 3090 GPU with 24GB memory)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# setup quantization config\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e19d86a5463410b83b5ef4ec0c515ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)  # quantized model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who are you?\n",
      "\n",
      "I am a 23-year-old graphic designer from the Netherlands. I’ve been working in the design industry for about 5 years now. I’ve worked for various design agencies and studios, but I’ve been freelancing for the past year and a half.\n",
      "\n",
      "What do you do?\n",
      "\n",
      "I specialize in branding, visual identity, and print design. I help businesses and individuals create a strong visual identity that sets them apart from their competitors. I design logos, business cards, brochures, websites, and other marketing materials.\n",
      "\n",
      "What inspired you to become a graphic designer?\n",
      "\n",
      "I’ve always been interested in art and design. In high school, I took a graphic design class and fell in love with it. I loved the idea of creating visual solutions to communicate ideas and solve problems. I went on to study graphic design in college and have been working in the industry ever since.\n",
      "\n",
      "What do you enjoy most about being a graphic designer?\n",
      "\n",
      "I enjoy the creative problem-solving aspect of graphic design. Every project is unique and presents its own challenges. I also enjoy the collaborative nature of working with clients to bring their vision to life.\n",
      "\n",
      "What is your design process like?\n",
      "\n",
      "My design process starts with research and understanding the client’s needs and goals. I then brainstorm ideas and concepts and create rough sketches or mood boards. I refine the concepts and create digital mockups, and then I present the designs to the client for feedback. I make revisions based on the client’s feedback and finalize the design.\n",
      "\n",
      "What are some challenges you face as a graphic designer?\n",
      "\n",
      "One challenge I face as a graphic designer is meeting tight deadlines while still delivering high-quality work. Another challenge is staying up-to-date with the latest design trends and technologies. It’s important to find a balance between staying current and maintaining a unique style.\n",
      "\n",
      "What advice would you give to someone who wants to become a graphic designer?\n",
      "\n",
      "My advice to someone who wants to become a graphic designer would be to invest in learning the fundamentals of design, such as typography, color theory, and composition. Practice as much as possible and build a strong portfolio. Networking and building relationships in the industry can also be valuable. And finally, be open to feedback and willing to learn and grow.\n"
     ]
    }
   ],
   "source": [
    "prompts = \"Who are you?\"\n",
    "input_ids = tokenizer(prompts, return_tensors=\"pt\").to(device)\n",
    "# print(len(input_ids.input_ids[0]))\n",
    "attention_mask = tokenizer(prompts, return_tensors=\"pt\").attention_mask.to(device)\n",
    "\n",
    "# Generate output using the model\n",
    "outputs = model.generate(\n",
    "    input_ids.input_ids,\n",
    "    num_return_sequences=1,  # number of different sequences to generate\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "# Decode the generated output\n",
    "outputs = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(outputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use CPU Offloading to a normal Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CPU offloading is a technique to offload the computation to a CPU. It is useful when the model is too large to fit in the GPU memory. We only move the layer that is needed for the current computation to the GPU and move it back to the CPU after the computation is done."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe4413fa2911483e906d0ae7523d32b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import cpu_offload\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = cpu_offload(model, execution_device=device)  # offload the model to the CPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/admin1/anaconda3/envs/accelerate/lib/python3.8/site-packages/transformers/generation/utils.py:1460: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on meta. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('meta') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who are you?\n",
      "\n",
      "I am a 23-year-old artist from the Netherlands. I’ve been drawing since I was a kid, but I started taking it more seriously around 18. I’ve been working as a freelance artist for a few years now.\n",
      "\n",
      "What do you create?\n",
      "\n",
      "I create digital illustrations, mostly of characters and creatures. I like to experiment with different styles and techniques, but I usually stick to a more detailed and colorful approach.\n",
      "\n",
      "What inspires you?\n",
      "\n",
      "I find inspiration in a lot of things, from nature and animals to movies, books, and other art. I also love to explore different cultures and mythologies. I think it’s important to keep an open mind and be curious about the world around you.\n",
      "\n",
      "What is your creative process like?\n",
      "\n",
      "My creative process usually starts with brainstorming ideas and sketching out rough concepts. I like to experiment with different compositions and color palettes to find what works best for the concept. Once I have a solid idea, I’ll start refining the details and adding textures and shading to bring the illustration to life.\n",
      "\n",
      "What tools do you use?\n",
      "\n",
      "I use a Wacom Intuos Pro tablet and Adobe Photoshop for creating my illustrations. I also use a variety of brushes and textures to add depth and texture to my work.\n",
      "\n",
      "What do you hope people take away from your art?\n",
      "\n",
      "I hope that people are able to find joy and inspiration in my art. I want to create work that is visually appealing and tells a story, whether it’s through the characters I draw or the themes I explore. Ultimately, I want to create art that brings a little bit of magic and wonder into people’s lives.\n"
     ]
    }
   ],
   "source": [
    "prompts = \"Who are you?\"\n",
    "input_ids = tokenizer(prompts, return_tensors=\"pt\").to(device)\n",
    "# print(len(input_ids.input_ids[0]))\n",
    "attention_mask = tokenizer(prompts, return_tensors=\"pt\").attention_mask.to(device)\n",
    "\n",
    "# Generate output using the model\n",
    "outputs = model.generate(\n",
    "    input_ids.input_ids,\n",
    "    num_return_sequences=1,  # number of different sequences to generate\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "# Decode the generated output\n",
    "outputs = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(outputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
